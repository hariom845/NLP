{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac6c96de",
   "metadata": {},
   "source": [
    "###\n",
    " Tokenization : Converting corpus/Documents into tokens "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8fcf86eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\asus\\venv\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: click in c:\\users\\asus\\venv\\lib\\site-packages (from nltk) (8.2.1)\n",
      "Requirement already satisfied: joblib in c:\\users\\asus\\venv\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\asus\\venv\\lib\\site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in c:\\users\\asus\\venv\\lib\\site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\asus\\venv\\lib\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e5712eab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Natural Language Toolkit (NLTK) is one of the largest Python libraries for performing various Natural Language Processing tasks.\n",
      "From rudimentary tasks such as text pre-processing to tasks like vectorized representation of text - NLTK's API has covered everything. \n",
      "In this article, we will accustom ourselves to the basics of NLTK and perform some crucial NLP tasks: Tokenization, Stemming, Lemmatization, and POS Tagging. \n"
     ]
    }
   ],
   "source": [
    "corpus=\"\"\"Natural Language Toolkit (NLTK) is one of the largest Python libraries for performing various Natural Language Processing tasks.\n",
    "From rudimentary tasks such as text pre-processing to tasks like vectorized representation of text - NLTK's API has covered everything. \n",
    "In this article, we will accustom ourselves to the basics of NLTK and perform some crucial NLP tasks: Tokenization, Stemming, Lemmatization, and POS Tagging. \"\"\"\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c09e207f",
   "metadata": {},
   "source": [
    "- paragrap(corpus)--->sentence(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8a122bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b0e64651",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "3\n",
      "Natural Language Toolkit (NLTK) is one of the largest Python libraries for performing various Natural Language Processing tasks.\n",
      "From rudimentary tasks such as text pre-processing to tasks like vectorized representation of text - NLTK's API has covered everything.\n",
      "In this article, we will accustom ourselves to the basics of NLTK and perform some crucial NLP tasks: Tokenization, Stemming, Lemmatization, and POS Tagging.\n"
     ]
    }
   ],
   "source": [
    "documents = sent_tokenize(corpus)\n",
    "print(type(documents))\n",
    "print(len(documents))\n",
    "for sentences in documents:\n",
    "    print(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad3626d",
   "metadata": {},
   "source": [
    "- paragraph-->words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ad71c54f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f50c9f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "73\n",
      "Natural\n",
      "Language\n",
      "Toolkit\n",
      "(\n",
      "NLTK\n",
      ")\n",
      "is\n",
      "one\n",
      "of\n",
      "the\n",
      "largest\n",
      "Python\n",
      "libraries\n",
      "for\n",
      "performing\n",
      "various\n",
      "Natural\n",
      "Language\n",
      "Processing\n",
      "tasks\n",
      ".\n",
      "From\n",
      "rudimentary\n",
      "tasks\n",
      "such\n",
      "as\n",
      "text\n",
      "pre-processing\n",
      "to\n",
      "tasks\n",
      "like\n",
      "vectorized\n",
      "representation\n",
      "of\n",
      "text\n",
      "-\n",
      "NLTK\n",
      "'s\n",
      "API\n",
      "has\n",
      "covered\n",
      "everything\n",
      ".\n",
      "In\n",
      "this\n",
      "article\n",
      ",\n",
      "we\n",
      "will\n",
      "accustom\n",
      "ourselves\n",
      "to\n",
      "the\n",
      "basics\n",
      "of\n",
      "NLTK\n",
      "and\n",
      "perform\n",
      "some\n",
      "crucial\n",
      "NLP\n",
      "tasks\n",
      ":\n",
      "Tokenization\n",
      ",\n",
      "Stemming\n",
      ",\n",
      "Lemmatization\n",
      ",\n",
      "and\n",
      "POS\n",
      "Tagging\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "words = word_tokenize(corpus)\n",
    "print(type(words))\n",
    "print(len(words))\n",
    "for word in words:\n",
    "    print(word) # ' is not seperated from the s (NLTK's)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ecb4c6",
   "metadata": {},
   "source": [
    "- sentences-->words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "99ca23d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Natural', 'Language', 'Toolkit', '(', 'NLTK', ')', 'is', 'one', 'of', 'the', 'largest', 'Python', 'libraries', 'for', 'performing', 'various', 'Natural', 'Language', 'Processing', 'tasks', '.']\n",
      "['From', 'rudimentary', 'tasks', 'such', 'as', 'text', 'pre-processing', 'to', 'tasks', 'like', 'vectorized', 'representation', 'of', 'text', '-', 'NLTK', \"'s\", 'API', 'has', 'covered', 'everything', '.']\n",
      "['In', 'this', 'article', ',', 'we', 'will', 'accustom', 'ourselves', 'to', 'the', 'basics', 'of', 'NLTK', 'and', 'perform', 'some', 'crucial', 'NLP', 'tasks', ':', 'Tokenization', ',', 'Stemming', ',', 'Lemmatization', ',', 'and', 'POS', 'Tagging', '.']\n"
     ]
    }
   ],
   "source": [
    "for sentence in documents:\n",
    "    print(word_tokenize(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "be9df069",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import wordpunct_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9e5478d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Natural',\n",
       " 'Language',\n",
       " 'Toolkit',\n",
       " '(',\n",
       " 'NLTK',\n",
       " ')',\n",
       " 'is',\n",
       " 'one',\n",
       " 'of',\n",
       " 'the',\n",
       " 'largest',\n",
       " 'Python',\n",
       " 'libraries',\n",
       " 'for',\n",
       " 'performing',\n",
       " 'various',\n",
       " 'Natural',\n",
       " 'Language',\n",
       " 'Processing',\n",
       " 'tasks',\n",
       " '.',\n",
       " 'From',\n",
       " 'rudimentary',\n",
       " 'tasks',\n",
       " 'such',\n",
       " 'as',\n",
       " 'text',\n",
       " 'pre',\n",
       " '-',\n",
       " 'processing',\n",
       " 'to',\n",
       " 'tasks',\n",
       " 'like',\n",
       " 'vectorized',\n",
       " 'representation',\n",
       " 'of',\n",
       " 'text',\n",
       " '-',\n",
       " 'NLTK',\n",
       " \"'\",\n",
       " 's',\n",
       " 'API',\n",
       " 'has',\n",
       " 'covered',\n",
       " 'everything',\n",
       " '.',\n",
       " 'In',\n",
       " 'this',\n",
       " 'article',\n",
       " ',',\n",
       " 'we',\n",
       " 'will',\n",
       " 'accustom',\n",
       " 'ourselves',\n",
       " 'to',\n",
       " 'the',\n",
       " 'basics',\n",
       " 'of',\n",
       " 'NLTK',\n",
       " 'and',\n",
       " 'perform',\n",
       " 'some',\n",
       " 'crucial',\n",
       " 'NLP',\n",
       " 'tasks',\n",
       " ':',\n",
       " 'Tokenization',\n",
       " ',',\n",
       " 'Stemming',\n",
       " ',',\n",
       " 'Lemmatization',\n",
       " ',',\n",
       " 'and',\n",
       " 'POS',\n",
       " 'Tagging',\n",
       " '.']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordpunct_tokenize(corpus) # here ' is seperated from the 's (NLTK's)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c635f18b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "tokenizer=TreebankWordTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "bcdf87ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Natural',\n",
       " 'Language',\n",
       " 'Toolkit',\n",
       " '(',\n",
       " 'NLTK',\n",
       " ')',\n",
       " 'is',\n",
       " 'one',\n",
       " 'of',\n",
       " 'the',\n",
       " 'largest',\n",
       " 'Python',\n",
       " 'libraries',\n",
       " 'for',\n",
       " 'performing',\n",
       " 'various',\n",
       " 'Natural',\n",
       " 'Language',\n",
       " 'Processing',\n",
       " 'tasks.',\n",
       " 'From',\n",
       " 'rudimentary',\n",
       " 'tasks',\n",
       " 'such',\n",
       " 'as',\n",
       " 'text',\n",
       " 'pre-processing',\n",
       " 'to',\n",
       " 'tasks',\n",
       " 'like',\n",
       " 'vectorized',\n",
       " 'representation',\n",
       " 'of',\n",
       " 'text',\n",
       " '-',\n",
       " 'NLTK',\n",
       " \"'s\",\n",
       " 'API',\n",
       " 'has',\n",
       " 'covered',\n",
       " 'everything.',\n",
       " 'In',\n",
       " 'this',\n",
       " 'article',\n",
       " ',',\n",
       " 'we',\n",
       " 'will',\n",
       " 'accustom',\n",
       " 'ourselves',\n",
       " 'to',\n",
       " 'the',\n",
       " 'basics',\n",
       " 'of',\n",
       " 'NLTK',\n",
       " 'and',\n",
       " 'perform',\n",
       " 'some',\n",
       " 'crucial',\n",
       " 'NLP',\n",
       " 'tasks',\n",
       " ':',\n",
       " 'Tokenization',\n",
       " ',',\n",
       " 'Stemming',\n",
       " ',',\n",
       " 'Lemmatization',\n",
       " ',',\n",
       " 'and',\n",
       " 'POS',\n",
       " 'Tagging',\n",
       " '.']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(corpus)\n",
    "# only last full stop is considered seperate only otherwise all full stops are kept with previous word"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
